{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqf1NsMKirKX",
        "outputId": "5c66039c-bdd5-472d-e30d-e98e31f5af9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "PylDjjtwf99c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "3PAs_Wg1gADD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d saniuzzamanrobin/ravdess-speech-all-audio-and-video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC3QjTzMolCc",
        "outputId": "8451fccb-25dd-431b-ad21-800b1124276c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ravdess-speech-all-audio-and-video.zip to /content\n",
            "100% 6.39G/6.39G [01:07<00:00, 97.1MB/s]\n",
            "100% 6.39G/6.39G [01:07<00:00, 102MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/ravdess-speech-all-audio-and-video.zip"
      ],
      "metadata": {
        "id": "o6nn7Y5fosVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "IpN0OsonowcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ],
      "metadata": {
        "id": "9rMdZyiipEwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "metadata": {
        "id": "TaEIJ8yVpKu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LfelH1NpT6u",
        "outputId": "03b6efad-74c5-41dc-804d-08b4331622ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fear','disgust','surprise']"
      ],
      "metadata": {
        "id": "18m_uFXKplTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_paths = os.listdir('/content/All Videos')"
      ],
      "metadata": {
        "id": "br6Ks4App9YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def prepare_all_videos(root_dir):\n",
        "    num_samples = len(video_paths)\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    labels = np.zeros(shape=(num_samples, 1), dtype=\"int\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for idx, path in tqdm(enumerate(video_paths)):\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "        labels[idx] = int(path.split('-')[2]) -1\n",
        "\n",
        "        temp_frame_mask = np.zeros(\n",
        "            shape=(\n",
        "                1,\n",
        "                MAX_SEQ_LENGTH,\n",
        "            ),\n",
        "            dtype=\"bool\",\n",
        "        )\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :], verbose=0,\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(\"/content/All Videos\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ],
      "metadata": {
        "id": "cSxgvYzhp5yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_oh = np.max(train_labels) + 1\n",
        "train_labels_oh=np.eye(train_labels_oh)[train_labels]\n",
        "train_labels_oh = train_labels_oh.reshape(-1,8)\n",
        "train_labels_oh.shape"
      ],
      "metadata": {
        "id": "Au7V-OYZuuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequence_model():\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    x = keras.layers.LSTM(32, return_sequences=True)(\n",
        "        frame_features_input, mask=mask_input\n",
        "    )\n",
        "    x = keras.layers.LSTM(16)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(emotions), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "\n",
        "seq_model = get_sequence_model()\n",
        "history = seq_model.fit(\n",
        "    [train_data[0], train_data[1]],\n",
        "    train_labels_oh,\n",
        "    validation_split=0.3,\n",
        "    epochs=EPOCHS,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8ND9kumSsh7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import moviepy.editor\n",
        "moviepy.editor.ipython_display('/content/All Videos/'+video_paths[0])"
      ],
      "metadata": {
        "id": "hAcXt0SqvOot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6EsSNKuxUCb",
        "outputId": "fbf80094-4d64-40ad-a8f1-a85f7204a1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = seq_model.predict([np.expand_dims(train_data[0][0],0),np.expand_dims(train_data[1][0],0)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJmW93qew3SM",
        "outputId": "d7696f59-a9fc-4076-f2b3-528970af6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions[np.argmax(pred)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kdSa3RLOxN7d",
        "outputId": "aa39927b-a25f-42ea-896f-62293efdde62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRs4DlAXxfhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}